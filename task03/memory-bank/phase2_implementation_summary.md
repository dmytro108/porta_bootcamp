# Phase 2 Implementation Summary

## Overview
Phase 2 implements the data generation and loading functionality for the MySQL cleanup benchmark project. This phase creates synthetic test data and populates all four method-specific tables with identical datasets for fair comparison.

## Completed Tasks

### 1. Data Loading Script (`db-load.sh`)

Created comprehensive data loading script with the following features:

#### Script Capabilities
- **CSV Generation**: Generates synthetic data with configurable row count
- **Bulk Loading**: Loads data into all four tables using batched INSERT statements
- **Validation**: Verifies MySQL connectivity, database, and table existence
- **Reusability**: Reuses existing CSV files to avoid regeneration
- **Summary Reports**: Displays data distribution statistics after loading

#### Command-Line Options
- `--rows N`: Number of rows to generate (default: 100000)
- `--db NAME`: Database name (default: cleanup_bench)
- `--force-regenerate`: Force CSV regeneration even if file exists
- `--verbose`: Enable detailed logging
- `-h, --help`: Display usage information

#### Example Usage
```bash
# Load default 100,000 rows
./run-in-container.sh db-load.sh

# Load custom number of rows with verbose output
./run-in-container.sh db-load.sh --rows 200000 --verbose

# Force regeneration of CSV
./run-in-container.sh db-load.sh --force-regenerate
```

### 2. Data Generation Strategy

#### Synthetic Data Model
- **id**: AUTO_INCREMENT (not in CSV, generated by MySQL)
- **ts**: Random DATETIME within last 20 days
  - Generated using: `NOW() - RANDOM_OFFSET` where offset ∈ [0, 20 days]
  - Ensures ~50% of rows are older than 10 days
- **name**: Random 10-character uppercase string (A-Z)
  - Generated using: ASCII characters 65-90
- **data**: Random integer in range [0, 10,000,000]

#### CSV Format
- **Location**: `task03/data/events_seed.csv`
- **Format**: `ts,name,data` (comma-separated)
- **No header**: Direct data rows only
- **Reusable**: Same CSV used for all four tables to ensure fair comparison

#### Example CSV Row
```
2025-11-15 14:23:45,ABCDEFGHIJ,5432198
```

### 3. Loading Methodology

#### Original Plan vs Implementation
- **Planned**: LOAD DATA LOCAL INFILE for fast bulk loading
- **Actual**: Batched INSERT statements due to `local_infile=OFF` in MySQL
- **Batch Size**: 1000 rows per INSERT statement
- **Transaction Management**: Auto-commit disabled during load, committed per batch

#### Loading Process
1. **Truncate** existing table data
2. **Generate SQL** with batched multi-row INSERT statements
3. **Execute** SQL file via mysql client
4. **Verify** row count matches expected
5. **Clean up** temporary SQL file

#### Performance Characteristics
- Efficient multi-row INSERT syntax: `INSERT INTO table VALUES (row1), (row2), ...`
- Transaction batching reduces commit overhead
- Temporary SQL file approach avoids shell escaping issues

### 4. Container Integration (`run-in-container.sh`)

Created wrapper script to execute task03 scripts inside the `db_master` container:

#### Why Needed
- MySQL runs in Docker container without exposed ports
- Database is only accessible from within container network
- task03 directory is mounted at `/home` in container

#### Features
- **Automatic environment loading**: Reads variables from task01/compose/.env
- **Environment passing**: Exports required variables to container
- **Container validation**: Checks if db_master is running
- **User-friendly**: Simple interface for running any task03 script

#### Example Usage
```bash
# Run data load script
./run-in-container.sh db-load.sh --rows 100000

# Run partition maintenance
./run-in-container.sh db-partition-maintenance.sh --dry-run

# Run cleanup benchmark (future)
./run-in-container.sh db-cleanup.sh
```

### 5. Validation and Testing

#### Test Results (1000 rows)
```
table_name              total_rows  rows_older_than_10d  oldest_ts            newest_ts
cleanup_partitioned     1000        506                  2025-10-31 10:44:45  2025-11-20 10:37:23
cleanup_truncate        1000        506                  2025-10-31 10:44:45  2025-11-20 10:37:23
cleanup_copy            1000        506                  2025-10-31 10:44:45  2025-11-20 10:37:23
cleanup_batch           1000        506                  2025-10-31 10:44:45  2025-11-20 10:37:23
```

#### Validation Checks ✓
- [x] All tables receive identical data (same CSV source)
- [x] Row counts match requested count
- [x] Timestamp range spans ~20 days as designed
- [x] Approximately 50% of rows older than 10 days (506/1000 = 50.6%)
- [x] Name values are 10-character uppercase strings
- [x] Data values within expected range
- [x] Script returns exit code 0 on success

### 6. Files Created

1. **`/home/padavan/repos/porta_bootcamp/task03/db-load.sh`**
   - Main data loading script (executable)
   - 350+ lines including functions and documentation

2. **`/home/padavan/repos/porta_bootcamp/task03/run-in-container.sh`**
   - Container execution wrapper (executable)
   - Handles environment and container orchestration

3. **`/home/padavan/repos/porta_bootcamp/task03/data/events_seed.csv`**
   - Generated CSV data (auto-created on first run)
   - Reused across multiple runs unless --force-regenerate

## Technical Implementation Details

### Environment Configuration
- **Database Connection**: Uses localhost when running inside container
- **Credentials**: Root user with password from environment variables
- **Environment Sources**: 
  - Prefers environment variables if already set
  - Falls back to sourcing .env file from multiple locations
  - Flexible for both host and container execution

### Error Handling
- Connection validation before operations
- Database and table existence checks
- Row count verification after load
- Warnings for mismatched row counts
- Non-zero exit codes on failures

### Logging
- Timestamped log messages
- Verbose mode for detailed operation tracking
- Progress indicators during CSV generation (dots every 10K rows)
- Summary statistics after completion

### Data Distribution
The random timestamp generation ensures:
- **Uniform distribution** across 20-day window
- **Natural variation** in date ranges
- **Predictable cleanup targets**: ~50% of data is "old" (>10 days)
- **Realistic test scenario**: Mix of recent and old data

## Design Decisions

### 1. INSERT vs LOAD DATA
**Decision**: Use batched INSERT statements instead of LOAD DATA LOCAL INFILE

**Rationale**:
- MySQL `local_infile` disabled by default for security
- Enabling would require MySQL server reconfiguration
- Batched INSERTs provide good performance (1000 rows/batch)
- More portable across different MySQL configurations
- Acceptable for test dataset sizes (100K-1M rows)

### 2. Single CSV for All Tables
**Decision**: Generate one CSV and reuse for all four tables

**Rationale**:
- Ensures **fair comparison** between cleanup methods
- All tables start with identical data distribution
- Faster setup (generate once, load four times)
- Easier to reproduce results
- Reduces disk space for test data

### 3. Container Execution Wrapper
**Decision**: Create `run-in-container.sh` instead of port mapping

**Rationale**:
- No need to modify existing Docker Compose configuration
- Leverages existing volume mount (`task03` → `/home`)
- More secure (no exposed MySQL ports)
- Consistent with task01 architecture
- Simpler for users (single command to run scripts)

### 4. AWK for CSV Generation
**Decision**: Use AWK for synthetic data generation

**Rationale**:
- Fast processing (C implementation)
- Built-in random number generation
- Good date/time handling with shell integration
- Single-pass generation
- No external dependencies (Python, Ruby, etc.)

## Performance Characteristics

### CSV Generation
- **Speed**: ~3-4 seconds for 1,000 rows
- **Estimated**: ~3-5 minutes for 100,000 rows
- **Memory**: Constant (streaming generation)
- **Disk**: ~40 bytes per row (CSV format)

### Data Loading (1000 rows)
- **Per table**: ~3 seconds
- **Total (4 tables)**: ~12 seconds
- **Throughput**: ~333 rows/second
- **Estimated for 100K rows**: ~5-7 minutes per table, ~25-30 minutes total

### Optimization Opportunities
For future scaling to millions of rows:
- Enable `local_infile` in MySQL for faster loading
- Use `mysqlimport` utility
- Increase batch size for INSERT statements
- Parallelize table loading
- Generate binary format instead of CSV

## Next Steps (Not in Phase 2)

The following items remain for future phases:

1. **Documentation**: Update task03/README.md with usage instructions
2. **Load Simulation**: Implement db-traffic.sh for background workload (Phase 3)
3. **Cleanup Methods**: Implement db-cleanup.sh with all four methods (Phase 5/6)
4. **Metrics Collection**: Add instrumentation for performance measurement (Phase 4)

## Known Limitations

1. **CSV Generation Time**: AWK with shell date commands is slower than native implementations
   - For very large datasets (>1M rows), consider Python/Perl generators
   
2. **Loading Performance**: INSERT statements slower than LOAD DATA
   - Acceptable for test datasets (<1M rows)
   - Consider enabling local_infile for production benchmarks

3. **No Progress Bar**: CSV generation only shows dots, not percentage
   - Could add progress percentage for better UX

4. **Fixed Batch Size**: 1000 rows hardcoded
   - Could make configurable for performance tuning

## Lessons Learned

1. **Container Networking**: Need to understand Docker network topology for database access
2. **MySQL Security Defaults**: Modern MySQL has stricter defaults (local_infile=OFF)
3. **Environment Flexibility**: Supporting both .env files and environment variables improves usability
4. **Data Quality**: Proper validation checks catch configuration issues early
5. **Reusability**: Wrapper scripts improve developer experience significantly

## Success Criteria Met

- [x] Script generates configurable number of rows
- [x] Data distributed across 20-day window
- [x] ~50% of data older than 10 days
- [x] All four tables populated with identical data
- [x] Validation confirms data quality
- [x] Error handling and logging implemented
- [x] Container integration working
- [x] Phase 2 checklist completed
